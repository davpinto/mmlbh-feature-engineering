---
title: "Desafio de Ciência de Dados Hotmart"
author: "David Pinto"
date: "`r Sys.Date()`"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      cache = TRUE, fig.align = "center", fig.path = "img/", 
                      dev = "png", dpi = 120, out.width = "100%")

## Load required packages
library(rmarkdown)
library(knitr)
```

<div style="text-align: justify;text-justify: inter-word;">

> Apresentação da solução do desafio em formato de relatório de dados reprodutível, usando RMarkdown. Todas as informações necessárias para reproduzir fielmente os resultados do experimento realizado estão contidas neste documento. Atualizando-se as bases de dados, este relatório é capaz de refletir as alterações com apenas um clique de botão.

</div>

## Introdução

<div style="text-align: justify;text-justify: inter-word;">

O desafio proposto consiste em um problema de classificação binária, visando a predição de abandono da plataforma pelos usuários da Hotmart. A base de dados contém informações de 13.036 usuários, caracterizados por 8 atributos (desconsiderando o id do usuário e a variável booleana de interesse), que podem ser complementados utilizando outras 3 bases de dados disponibilizadas. Os detalhes da utilização das demais bases serão discutidos nas seções seguintes.

Todos os experimentos foram executados em uma máquina com Intel Core i7-6700HQ, 16 GB RAM DDR4 e sistema operacional Ubuntu 16.04.1 LTS 64-bits. A ferramenta escolhida para manipulação, formatação, limpeza e visualização dos dados, assim como a criação de novas features, foi a linguagem estatística `R` na versão `3.3.1`. Essa escolha se deve ao fato do `R`, escolhendo-se corretamente as bibliotecas, ser hoje a melhor linguagem para manipulação e visualização de dados, e também a melhor ferramenta para documentação experimental. As bibliotecas do bundle `tidyverse` possuem backend em C++ e permitem manipular massas de dados de até 1 GB com facilidade. Mais detalhes sobre essas ferramentas podem ser encontradas no meu Github: [Análise Exploratória de Dados usando o R](https://github.com/davpinto/mmlbh-eda).

Para ajuste dos modelos de *machine learning* foi utilizada a biblioteca `H2O.ai`, que além de permitir trabalhar com bases de dados de médio e grade porte, possibilita um grande controle sobre os parâmetros dos algoritmos. Adicionalmente, a biblioteca permite especificar o máximo de memória RAM que será consumido no processamento, assim como o número máximo de threads que serão utilizadas. Para o problema em questão, limitamos o processamento a 4 threads e a 4 GB de RAM. Todos os modelos foram ajustados em poucos segundos.

</div>

## Organização do Repositório

<div style="text-align: justify;text-justify: inter-word;">

A fim de garantir a reprodutibilidade de todos os resultados alcançados, os arquivos e códigos precisam ser organizados de forma clara e robusta. Para o presente experimento, foi adotada a seguinte estrutura de organização:

- O diretório é um projeto do RStudio, IDE escolhida como plataforma de trabalho.
- No diretório raiz:
    - Arquivo `README.Rmd` descrevendo o experimento. Este arquivo gera uma versão em `markdown`, para ser automaticamente renderizada no Github e/ou no Bitbucket, e outra em `HTML`, com mais recursos visuais.
    - Arquivo `requirements.txt` listando todas as bibliotecas utilizadas e suas respectivas versões.
    - Arquivo `references.bib` contendo todas as referências bibliográficas que auxiliaram na experimentação.
    - Arquivo `playground.R`, que consiste em um script experimental de solução do problema, que será detalhado no decorrer deste documento.
- Pasta `./data` contendo as bases de dados no formato `CSV`.

</div>

### Bibliotecas Utilizadas

Para solução do desafio foram utilizadas as seguintes bibliotecas do `R`:

```{r}
## Load required packages
library(readr)      ## To import data
library(magrittr)   ## To use the pipeline operator %>% 
library(tidyr)      ## To clean and format data
library(purrr)      ## To functional programming
library(dplyr)      ## To manipulate data
library(lubridate)  ## To manipulate dates
library(ggplot2)    ## To visualize data
library(reshape2)   ## To format data before ploting
library(hrbrthemes) ## To customize graph themes
library(h2o)        ## GLM and Random Forest algorithms
library(fastknn)    ## KNN algorithm to do non-linear feature extraction
```

## Manipulação e Preparação dos Dados

<div style="text-align: justify;text-justify: inter-word;">

Primeiramente, vamos carregar as bases de dados e fazer uma inspeção em cada uma delas:

```{r}
## Import datasets
products <- read_delim(file = "data/produtos_cadastrados.csv", delim = ";", 
                       col_names = TRUE, col_types = "iii")
product.niches <- read_delim(file = "data/produtos_cadastrados_por_nicho.csv", 
                             delim = ";", col_names = TRUE, col_types = "ici")
affiliates <- read_delim(file = "data/quantidade_de_afiliados.csv", 
                         delim = ";", col_names = TRUE, col_types = "ii")
users <- read_delim(file = "data/performance_dos_clientes.csv", 
                    delim = ";", col_names = TRUE, col_types = "icccccccii")

## Preview datasets
glimpse(users)
glimpse(affiliates)
glimpse(products)
glimpse(product.niches)
```

Das bases extra disponíveis, a única que não pode ser incorporada diretamente à `users` é a base `product.niches`, pois o mesmo `user_id` aparece em múltiplas instâncias de dados. O mais adequado a se fazer é transformar a coluna `niches` em várias colunas numéricas, uma para cada categoria. Dessa forma teremos uma nova tabela que não repete os ids de usuário em diferentes tuplas. A biblioteca `tidyr` permite fazer essa operação de forma bem simples:

```{r}
## Transform column niches into many numeric columns
product.niches <- product.niches %>% 
   spread(key = niche, value = qtd_produtos, fill = as.integer(0), 
          convert = FALSE) %>% 
   set_names(sprintf("qtd_%s", names(.))) %>% 
   rename(user_id = qtd_user_id)
glimpse(product.niches)
```

Agora podemos unir todas as bases, utilizando a biblioteca `dplyr`, para formar um conjunto de dados único, contendo todas as informações que temos disponíveis a respeito dos usuários:

```{r}
## Merge datasets
dt <- users %>% 
   left_join(affiliates, by = "user_id") %>% 
   left_join(products, by = "user_id") %>% 
   left_join(product.niches, by = "user_id")
glimpse(dt)
```

Nosso *dataset* tem agora 38 variáveis. Devido às operações de `join`, geramos alguns *missing values* nos dados, pois nem todos os ids da tabela `user` encontraram correspondência nas demais tabelas. Além disso, como resultado da expansão que fizemos na base `product.niches`, geramos variáveis esparsas (com alta concentração de zeros). Teremos então que trabalhar com dois problemas desafiadores no contexto de *machine learning*: valores faltantes e esparsidade nos dados. Trataremos desse assunto mais à frente, na análise exploratória e quando estivermos ajustando os modelos de predição.

As variáveis referentes ao faturamento dos usuários estão em formato de texto. Precisamos convertê-las para o formato numérico:

```{r}
## Format earning columns
dt <- dt %>% 
   set_names(trimws(names(.))) %>% 
   mutate_each(funs(as.numeric(trimws(gsub("R\\$", "", .)))), 
               starts_with("faturamento_"))
glimpse(select(dt, starts_with("faturamento_")))
```

Uma última manipulação que deve ser feita nos dados é a conversão das variáveis do tipo data para um formato adequado no `R`. Para isso utilizaremos a biblioteca `lubridate`, que será muito útil também na etapa de *feature engineering*:

```{r}
## Format date columns
dt <- dt %>% 
   mutate_each(funs(dmy), starts_with("data_"))
glimpse(select(dt, starts_with("data_")))
```

</div>

Agora estamos aptos a trabalhar com os dados.

## Feature Engineering

<div style="text-align: justify;text-justify: inter-word;">

Nesta fase utilizaremos as variáveis de data para gerar novos insights que podem ajudar a entender o que motiva os usuários a abandonarem a plataforma. Primeiramente vamos dividir os usuários em Cohorts para avaliar a taxa de evasão ao longo do tempo. Criaremos Cohorts trimestrais e dividiremos os usuários de acordo com a data que ingressaram na plataforma:

```{r fig.height = 8}
## Create cohorts
dt <- dt %>% 
   mutate(
      cohort = quarter(data_cadastro_plataforma, with_year = TRUE),
      cohort = factor(cohort, levels = sort(unique(cohort), decreasing = TRUE))
   )

## Plot leaving rate inside each cohort
g <- dt %>% 
   group_by(cohort) %>% 
   summarize(taxa_abandono = sum(saiu_da_plataforma == 1) / n()) %>% 
   ungroup() %>% 
   ggplot(aes(x = cohort, y = taxa_abandono, fill = taxa_abandono)) +
   geom_col(width = 0.8, alpha = 0.8) +
   scale_fill_distiller(name = "Taxa", palette = "Spectral", limits = c(0, 1)) +
   scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
   guides(fill = guide_colorbar(barwidth = 0.5, barheight = 15)) +
   coord_flip() +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Cohort", y = "Taxa de abandono da plataforma", 
        title = "Taxa de Abandono", 
        subtitle = "Percentual de abonadono por Cohort de tempo")
plot(g)
```

Percebe-se que nos Cohorts finais a taxa de evasão caiu. Mas isso era esperado, pois o abandono requer tempo e pode ser que daqui alguns meses os usuários das Cohorts mais atuais cheguem a uma taxa de evasão semelhante à de Cohorts anteriores. Esse gráfico nos mostra na verdade que estamos trabalhando com variáveis acumuladas, ou seja, quem for mais velho na base naturalmente apresentará valores maiores para as variáveis: maior faturamento total, maior quantidade de produtos vendidos, maior número de afiliados, e assim por diante. Por isso, temos que transformar todas as variáveis em taxas, ou seja, dividir todos os valores pelo número de dias que o usuário está na base. Assim, poderemos fazer uma comparação mais justa entre os usuários e, consequentemente, poderemos entender melhor o que de fato leva um usuário a sair da plataforma.

```{r}
## Transform variables into rates
dt <- dt %>% 
   mutate(
      dias_vendendo = as.integer(difftime(
         data_ultima_venda, data_primeira_venda, units = "days"
      )),
      tempo_primeira_venda = as.integer(difftime(
         data_primeira_venda, data_cadastro_plataforma, units = "days"
      )),
      faturamento_diario_geral = faturamento_total / dias_vendendo,
      faturamento_diario_ultimos12meses = faturamento_ultimos12meses / (12 * 30),
      faturamento_diario_ultimos6meses = faturamento_ultimos6meses / (6 * 30),
      faturamento_diario_ultimos3meses = faturamento_ultimos3meses / (3 * 30)
   ) %>% 
   filter(dias_vendendo > 0) %>% 
   mutate_each(funs(normalize = . / dias_vendendo), starts_with("qtd_")) %>% 
   select(-user_id, -data_cadastro_plataforma, -data_primeira_venda, 
          -data_ultima_venda, -dias_vendendo, -faturamento_total, 
          -faturamento_ultimos12meses, -faturamento_ultimos6meses, 
          -faturamento_ultimos3meses, -cohort)
glimpse(dt)
```

Calculamos então o número de dias entre a primeira e a última venda e o aplicamos para normalizar as variáveis. Removemos também aqueles usuários cujo número de dias é zero, pois eles não fornecem informação útil para nosso estudo. Por fim, criamos uma nova variável, que conta quantos dias o usuário demorou para fazer sua primeira venda desde que entrou na plataforma.

A base de dados final, que será utilizada pelos algoritmos de aprendizado, contém 34 variáveis numéricas de entrada mais a variável de interesse `saiu_da_plataforma`.

</div>

## Análise Exploratória dos Dados

<div style="text-align: justify;text-justify: inter-word;">

Antes de partir para o treinamento dos modelos de predição, é importante fazer uma inspeção minuciosa nos dados, a fim de entender melhor o tipo de problema que temos em mãos e também o conteúdo e qualidade das variáveis explanatórias disponíveis para aprendizado do problema.

Primeiramente, vamos analisar nossa variável de interesse:

- `r sprintf("%.1f%%", 100 * sum(dt$saiu_da_plataforma == 0) / nrow(dt))` de rótulos negativos (usuários que não abandonaram a plataforma).
- `r sprintf("%.1f%%", 100 * sum(dt$saiu_da_plataforma == 1) / nrow(dt))` de rótulo positivos (usuários que saíram da plataforma).

Portanto, existe um certo desbalanceamente entre as classes do problema, mas não é uma difereça muito expressiva. De qualquer forma, avaliaremos os modelos utilizando como métrica a área da curva ROC, ou AUC, que é uma métrica mais adequada que a acurácia para problemas de classificação desbalanceados [@castro-2011].

Tendo analisado a variável de saída, a inspeção das variáveis explanatórias será guiada por algumas das diretrizes apontadas neste guia, fortemente recomendado para aqueles que trabalham com modelos preditivos: [Preparing Data for Predictive Analytics](http://winvector.github.io/DataPrep/EN-CNTNT-Whitepaper-Data-Prep-Using-R.pdf). Basicamente, analizaremos a concentração de missing values e a concentração de zeros (esparsidade) em cada variável. Em resumo, alta concentração de valores faltantes e alta esparsidade dificultam o aprendizado, pois variáveis com algum desses problemas carregam pouca informação no que tange o discernimento entre as classes e dificultam o aprendizado devido à escassez de valores válidos disponíveis.

A concentração de valores faltantes na base de dados de um modo geral é considerável, correspondendo a `r sprintf("%.1f%%", 100 * sum(is.na(dt)) / (nrow(dt) * (ncol(dt) - 1)))` dos valores disponíveis. O gráfico a seguir ilustra a distribuição da proporção de missing values nas variáveis de entrada:

```{r}
## Check missing values
input.names <- setdiff(names(dt), "saiu_da_plataforma")
na.prop <- map_int(dt[, input.names], function(x) sum(is.na(x))) / nrow(dt)
g <- data_frame(x = na.prop) %>% 
   ggplot(aes(x = x, fill = ..count..)) +
   geom_histogram(bins = 5, alpha = 0.8, size = 0.5, color = "white") +
   scale_fill_distiller(guide = "none", palette = "Spectral") +
   scale_y_continuous(breaks = seq(0, 30, by = 5), limits = c(0, 30)) +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Proporção de valores faltantes", y = "Frequência",
        title = "Valores faltantes", 
        subtitle = "Distribuição da proporção de valores faltantes nas variáveis de entrada")
plot(g)
```

Nota-se que nenhuma variável ultrapssa os 60% de concentração de *missing values*. Portanto, não precisamos remover nenhuma delas, pois todas carregam um percentual de informação relevante.

Em relação à esparsidade das variáveis de entrada, o gráfico a seguir ilustra a distribuição da proporcão de zeros nas mesmas:

```{r}
## Check sparsity
zero.prop <- map_int(dt[, input.names], function(x) sum(x == 0, na.rm = TRUE)) / nrow(dt)
g <- data_frame(x = zero.prop) %>% 
   ggplot(aes(x = x, fill = ..count..)) +
   geom_histogram(bins = 30, alpha = 0.8, size = 0.5, color = "white") +
   scale_fill_distiller(guide = "none", palette = "Spectral") +
   scale_y_continuous(breaks = seq(0, 15, by = 2.5), limits = c(0, 15)) +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Concentração de zeros", y = "Frequência",
        title = "Esparsidade", 
        subtitle = "Distribuição da concentração de zeros nas variáveis de entrada")
plot(g)
```

Nota-se que um número considerável de variáveis ultrapassam os 75% de concentração de zeros, mas carregam ainda um percentual considerável de informação válida para o aprendizado. Novamente, optaremos por manter todas as variáveis.

Outro problema comum em dados multivariados é a **multicolinearidade**, ou seja, a presença de variáveis redundantes. Variáveis que, por maior que seja seu poder explanatório, carregam essencialmente a mesma informação a respeito das classes, além de desnessárias ao mesmo tempo no modelo, prejudicam as estimativas de mínimos quadrados em modelos de regressão, pois a solução dos coeficientes dos modelos deixa de ser única. Isso pode ser resolvido utilizando-se alguma técnica de regularização como Ridge regression, Lasso ou Elastic-net. No entanto, se existirem muitos pares de variáveis com alta correlação linear, a remoção de uma variável de cada par pode reduzir bastante o tempo computacional de treinamento dos modelos. O gráfico a seguir ilustra a correlação linear (coeficiente de Pearson) entre os pares de variáveis de entrada dos dados em questão:

```{r fig.height=6}
## Correlation matrix
var.mean <- map(dt[, input.names], mean, na.rm = TRUE)
cor.matrix <- dt %>% 
   replace_na(replace = var.mean) %>% 
   select(one_of(input.names)) %>% 
   cor() %>% 
   melt()
g <- ggplot(
   data = cor.matrix, 
   aes(x = Var1, y = Var2, fill = abs(value), color = abs(value), 
       alpha = abs(value))
) + 
   geom_tile(size = 0.1) +
   scale_fill_distiller(name = "Abs Corr.", palette = "Spectral", limits = c(0, 1)) +
   scale_color_distiller(guide = "none", palette = "Spectral", limits = c(0, 1)) +
   scale_alpha(guide = "none", range = 0.8, 1) +
   guides(fill = guide_colorbar(barwidth = 0.5, barheight = 15)) +
   theme_ipsum(axis_title_size = 12, axis_text_size = 8) +
   theme(axis.title.x = element_blank(), axis.text.x = element_blank(),
         axis.ticks.x = element_blank()) +
   labs(y = "Variáveis", title = "Matriz de correlação", 
        subtitle = "Valor absoluto da correlação linear entre os pares de variáveis de entrada")
plot(g)
```

Para calcular os índices de correlação, os valores faltantes foram substituídos pela médias das respectivas variáveis às quais pertencem. Nota-se que os valores dos coeficientes de correlação não ultrapassam 0.5, por isso não devemos ter grandes preocupações com multicolinearidade.

</div>

## Ranqueamento das Variáveis usando Correlação de Pearson

<div style="text-align: justify;text-justify: inter-word;">

Uma maneira simples de checar quais variáveis de entrada explicam melhor a variável de saída, consiste em determinar o coeficiente de correlação linear entre as primeiras e a última, da seguinte forma:

```{r}
## Correlation with the target variable
target.cor <- map_dbl(dt[, input.names], function(x, y) {
   x <- x / abs(max(x, na.rm = TRUE))
   abs(stats::cor(x[is.finite(x)], y[is.finite(x)], method = "pearson"))
}, y = dt$saiu_da_plataforma)

## Plot correlation rank
g <- data_frame(
   y = target.cor, x = input.names
) %>% 
   mutate(x = factor(x, levels = x[order(y)])) %>% 
   top_n(15, y) %>% 
   ggplot(aes(x = x, y = y, fill = y)) +
   geom_col(width = 0.8, alpha = 0.8) +
   scale_fill_distiller(name = "Cor", palette = "Spectral", limits = c(0, 1)) +
   scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
   guides(fill = guide_colorbar(barwidth = 0.5, barheight = 15)) +
   coord_flip() +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Variável", y = "Coeficiente de correlação linear absoluto", 
        title = "Correlação com a variável de interesse", 
        subtitle = "Top 15 variáveis mais correlacionadas")
plot(g)
```

Fica bem nítido que a correlação linear entre as variáveis de entrada e a variável de saída é muito baixa. Nenhuma variável superou os 0.15 de correlação com a saída. Logo, nenhuma das variáveis parece explicar linearmente a saída dos usuários da plataforma. Analisando, por exemplo, a variável mais correlacionada:

```{r}
## Best feature distribution grouped by the target variable
g <- dt %>% 
   mutate(saiu_da_plataforma = factor(saiu_da_plataforma, levels = c(0,1))) %>% 
   ggplot(aes(x = qtd_produtos_ja_vendeu / abs(max(qtd_produtos_ja_vendeu)), 
              fill = saiu_da_plataforma, color = saiu_da_plataforma)) +
   geom_density(adjust = 5, size = 0.5, alpha = 0.6) +
   scale_y_continuous(breaks = seq(0, 20, by = 2.5), limits = c(0, 20)) +
   scale_x_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
   scale_fill_brewer(name = "Abandonou", palette = "Dark2") +
   scale_color_brewer(name = "Abandonou", palette = "Dark2") +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Média diária de vendas (Normalizado)", y = "Densidade",
        title = "Distribuição da variável mais correlacionada",
        subtitle = "Variável mais correlacionada agrupada pela variável de interesse") 
plot(g)
```

Percebe-se que não existe separação entre as classes dentro dos valores da variável de entrada analisada.

</div>

## Ranqueamento das Variáveis usando Modelos GLM

<div style="text-align: justify;text-justify: inter-word;">

O ranquemento anterior é univariável, ou seja, ele não leva em consideração a contruibuição conjunta das variáveis na explicação da variável de saída. Então, vamos partir agora para uma modelo linear multivariável do tipo GLM, conhecido como Regressão Logística, que corresponde a um classificador linear binário. Utilizaremos regularização L2 para garantir uma maior correspondência entre os valores dos coeficientes estimados da regressão e a importância das respectivas variáveis na explicação da variável de interesse.

```{r results='hide'}
## Instantiate H2O cluster
h2o.init(max_mem_size = '4G', nthreads = 4)
h2o.removeAll()

## Import data to h2o
data.hex <- dt %>% 
   mutate(saiu_da_plataforma = factor(saiu_da_plataforma, levels = c(0, 1))) %>% 
   as.h2o(destination_frame = "data_hex")

## Fit GLM
glm.model <- h2o.glm(x = input.names, y = "saiu_da_plataforma", 
                     training_frame = data.hex, nfolds = 5, family = "binomial", 
                     solver = "IRLSM", alpha = 0, standardize = TRUE, 
                     intercept = TRUE, max_iterations = 1e3, seed = 2020,
                     lambda_search = TRUE, nlambdas = 50)
```

Previamente ao treinamento dos modelos GLM, a biblioteca `h2o` preenche automaticamente os valores faltantes usando a média das variáveis. Para avaliar o modelo gerado foi utilizada validação-cruzada 5-fold estratificada (mantendo a mesma proporção entre classes dentro de cada fold). A AUC de validação foi:

```{r}
## AUC from cross-validation
h2o.auc(glm.model@model$cross_validation_metrics)
```

Ranqueando as variáveis de acordo com o os coeficientes da regressão, temos a seguinte orderm de importância:

```{r}
# Variable importance given by the GLM model
glm.varimp <- h2o.varimp(glm.model)
head(glm.varimp, 10)
```

</div>

### Mapeamento Não-Linear dos Dados usando KNN

<div style="text-align: justify;text-justify: inter-word;">

Vimos anteriormente que nenhuma variável de entrada se relaciona linearmente com a variável de saída. Portanto, um modelo linear talvez não seja a opção mais apropriada para aprendizado do problema.

Para averiguar se existe uma relação não-linear entre as variáveis explanatórias e a variável dependente, utilizaremos a biblioteca [fastknn](http://davpinto.com/fastknn/), de minha autoria. Particularmente, aplicaremos a função `knnExtract()` para fazer um mapeamento não-linear supervisionado dos dados de entrada e gerar novas features em um espaço linearizado. Esse método de extração de features foi desenvolvido por mim, inspirado por algumas soluções vencedores de competições do Kaggle.

```{r results='hide'}
## KNN features
x <- dt %>% 
   replace_na(replace = var.mean) %>% 
   select(one_of(input.names)) %>% 
   data.matrix()
new.data <- knnExtract(xtr = x, ytr = factor(dt$saiu_da_plataforma, c(0,1)), 
                       xte = x, k = 3, normalize = "maxabs", folds = 5, 
                       nthread = 6)
dt.knn <- bind_cols(as.data.frame(new.data$new.tr), select(dt, saiu_da_plataforma))
knn.vars <- colnames(new.data$new.tr)
```

O método gera `k * C` novas features, onde `k` é o número de vizinhos escolhido e `C` é o número de classes do problema. Vamos analisar esse novo espaço usando um *scatter plot*:

```{r}
## Plot KNN features
ggplot(dt.knn, aes(x = knn1, y = knn6, color = factor(saiu_da_plataforma, c(0,1)))) +
   geom_point(size = 1, alpha = 0.8) +
   scale_color_brewer(name = "Abandonou", palette = "Dark2") +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "KNN_01", y = "KNN_06")
```

Nota-se uma separação considerável entre as classes. Vamos ver então como essas novas variáveis se correlacionam com a variável de saída:

```{r}
## Correlation with the target variable
target.cor <- map_dbl(dt.knn[, knn.vars], function(x, y) {
   x <- x / abs(max(x, na.rm = TRUE))
   abs(stats::cor(x[is.finite(x)], y[is.finite(x)], method = "pearson"))
}, y = dt.knn$saiu_da_plataforma)

## Plot correlation rank
g <- data_frame(
   y = target.cor, x = knn.vars
) %>% 
   mutate(x = factor(x, levels = x[order(y)])) %>% 
   ggplot(aes(x = x, y = y, fill = y)) +
   geom_col(width = 0.8, alpha = 0.8) +
   scale_fill_distiller(name = "Cor", palette = "Spectral", limits = c(0, 1)) +
   scale_y_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
   guides(fill = guide_colorbar(barwidth = 0.5, barheight = 15)) +
   coord_flip() +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Variável", y = "Coeficiente de correlação linear absoluto", 
        title = "Correlação com a variável de interesse", 
        subtitle = "Correlação das variáveis KNN com a variável de saída")
plot(g)
```

A correlação teve um aumento considerável, chegando a quase 0.55 para 3 das variáveis. Vamos analisar a distribuição da variável mais correlacionada:

```{r}
## Density of the best knn feature
g <- dt.knn %>% 
   mutate(saiu_da_plataforma = factor(saiu_da_plataforma, levels = c(0,1))) %>% 
   ggplot(aes(x = knn5, fill = saiu_da_plataforma, color = saiu_da_plataforma)) +
   geom_density(adjust = 5, size = 0.5, alpha = 0.6) +
   scale_y_continuous(breaks = seq(0, 20, by = 2.5), limits = c(0, 20)) +
   scale_x_continuous(breaks = seq(0, 1, by = 0.1), limits = c(0, 1)) +
   scale_fill_brewer(name = "Abandonou", palette = "Dark2") +
   scale_color_brewer(name = "Abandonou", palette = "Dark2") +
   theme_ipsum(axis_title_size = 12) +
   labs(x = "Variável extraída com KNN", y = "Densidade",
        title = "Distribuição da variável KNN",
        subtitle = "Segmentação da variável extraída com KNN de acordo com a variável de interesse") 
plot(g)
```

A separabilidade obtida é muito superior à da variável original mais correlacionada.

Agora vamos treinar um modelo GLM usando as features geradas pelo KNN, assim como sugerido neste tutorial: [Show to GLM what KNN sees](https://www.kaggle.com/davidpinto/fastknn-show-to-glm-what-knn-see-0-96).

```{r results='hide'}
## Import data to h2o
knn.hex <- dt.knn %>% 
   mutate(saiu_da_plataforma = factor(saiu_da_plataforma, levels = c(0, 1))) %>% 
   as.h2o(destination_frame = "knn_hex")

## Fit GLM
glm.model <- h2o.glm(x = knn.vars, y = "saiu_da_plataforma", 
                     training_frame = knn.hex, nfolds = 5, family = "binomial", 
                     solver = "IRLSM", alpha = 0, standardize = TRUE, 
                     intercept = TRUE, max_iterations = 1e3, seed = 2020,
                     lambda_search = TRUE, nlambdas = 50)
```

Nesse caso, a AUC de validação foi:

```{r}
## AUC from cross-validation
h2o.auc(glm.model@model$cross_validation_metrics)
```

Um desempenho muito superior ao do aprendizado com os dados originais.

Esse método de extração não-linear de características é muito poderoso e pode ser utilizado em produção para predizer a saída da plataforma, a fim de aumentar a performance preditiva. No entanto, ele não nos confere interpretabilidade no que se refere ao entedimento de qual variável de entrada mais contrbui para saída dos usuários. Então teremos que continuar explorando os dados para encontrar a resposta.

</div>

## Ranqueamento das Variáveis usando Random Forests

<div style="text-align: justify;text-justify: inter-word;">

Vamos utilizar agora um modelo não-linear multivariável que oferece uma série de vantagens no aprendizado do problema, o algoritmo Random Forest (RF). Várias propriedades do problema em questão tornam favorável a escolha por modelos baseados em árvores de decisão [@breiman-1984], devido às seguintes vantagens que tais classficadores oferecem:

- São insensíveis à escala das variáveis, ou seja, não é preciso normalizar as variáveis numéricas antes de treinar os modelos.
- Conseguem lidar com valores faltantes. Para as variáveis categóricas, os valores faltantes são considerados uma nova categoria. Já paras as variáveis numéricas, o processo de construção da árvore requer a discretização das mesmas, dessa forma os valores faltantes podem novamente serem considerados como uma nova categoria.
- São modelos não-paramétricos que, apesar de não terem a mesma interpretabilidade que modelos GLM, são muitos simples e intuitivos de construir.
- Conseguem criar interações entre a variáveis automaticamente, enquanto que na maior parte dos algoritmos de aprendizado essas interações devem ser testadas e implementadas previamente ao treinamento dos modelos.

A opção por um comitê (ensemble) de árvores de decisão se deve ao fato de apresentar maior poder de generalização e consequentemente maior performance preditiva que uma árvore de decisão sozinha. Além disso, consegue lidar melhor com alta dimensão, pois é possível treinar cada membro do ensemble com uma amostra aleatória consideravelmente menor do conjunto de variáveis originais. Amostrando aleatoriamente as variáveis e as instâncias de treinamento que serão entregues a cada árvore de decisão, o comitê consegue aumentar a diversidade dos membros, gerando pontos de vistas variados no aprendizado dos dados. Esses aumento de diversidade, dado um número suficiente de membros, reduz o bias e também a variância na decisão do comitê. Por isso que hoje é muito comum ver empresas montando equipes multidisciplinares, pois dá resultados mais positivos que reunir grandes gênios com formas semelhantes de pensar. Esse é o mesmo princípio que rege o algoritmo Random Forest. Por fim, o RF é um método muito recomendado para ranqueamento de features [@louppe-2013], pois consegue medir a importância de cada feature para cada árvore e depois gerar um índice de importância geral da variável para o comitê.

```{r results='hide'}
## Fit Random Forest
rf.model <- h2o.randomForest(
   x = input.names, y = "saiu_da_plataforma", training_frame = data.hex, 
   nfolds = 5, seed = 2020, ntrees = 500, stopping_rounds = 2, 
   stopping_metric = 'AUC', stopping_tolerance = 1e-5, 
   score_each_iteration = FALSE, score_tree_interval = 5, 
   max_depth = 8, min_rows = 32, sample_rate = 0.632, 
   mtries = 16, col_sample_rate_per_tree = 0.632, nbins = 8, nbins_cats = 8,
   histogram_type = "Random"
)
```

A biblioteca `h2o` permite configurar uma política de *early stopping* (parar de crescer o comitê assim que a performance de validação deixar de aumentar), por isso escolhemos um valor alto, 500 no caso, para a quantidade de árvores e deixamos que o próprio algoritmo identificasse o tamanho ideal do comitê.

A AUC de validação do comitê, formado por 42 árvores de decisão, foi:

```{r}
## Cross-validation AUC
h2o.auc(rf.model@model$cross_validation_metrics)
```

Uma performance muito superior à do modelo GLM. Quanto ao ranqueamento das variáveis, temos:

```{r}
## Variable importance given by the RF model
rf.varimp <- h2o.varimp(rf.model)
head(select(rf.varimp, variable, scaled_importance), 10)
```

O ranking divergiu daquele que foi obtido com o modelo GLM. Devido à performance preditiva alcançada com o RF, é mais sensato confiar no ranking que o último gerou.

```{r include=FALSE}
## Close h2o cluster
h2o.shutdown(prompt = FALSE)
```

</div>

## Conclusões

<div style="text-align: justify;text-justify: inter-word;">

A inspeção dos dados apontou para a necessidade de um modelo de aprendizado não-linear multivariado para entender a relação dos atributos dos usuários com os eventos de abandono da plataforma. O modelo ajustado com o algoritmo Random Forest apresentou um erro de predição muito baixo, demonstrando que aprendeu de fato a relação das variáveis de entrada com a variável de saída.

De acordo ainda com o algoritmo Random Forest, o que mais influencia a saída da plataforma é:

- O faturamento diário do usuário, em especial a média do último semestre.
- A quantidade de produtos vendidos diariamente, em especial produtos de 3 nichos específicos: `tnYmniXtrYtnY`, `strXdnXcisum` e `spihsnoitXlYr`.
- O número de dias demandado para fazer a primeira venda na plataforma.

</div>

******

## References
